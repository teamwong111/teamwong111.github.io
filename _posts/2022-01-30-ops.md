---
layout:     post
title:      "手把手教你如何高效地在 MMCV 中贡献算子"
subtile:    ""
date:       2022-01-30 14:00:00
author:     "jz wong"
header-img: "img/post-bg-2015.jpg"
tags:
    - PyTorch
---

> “文章由 jz wong 在实习期间撰写并发布在 OpenMMLab 官方账号，在此备份”

## 前言

不知道大家在使用 MMCV 的过程中有没有遇到这种情况：MMCV 没有提供自己需要的 CPU/CUDA 算子，于是希望提一个 PR（Pull Request），将这个算子加入 MMCV，但是又不知从何处下手。本文以最简单的 TensorAdd 算子为例，向大家展示为 MMCV 贡献算子的全过程，希望能够帮助大家更好地理解 MMCV 算子的目录结构以便更高效地贡献算子。 注意：如果您不太了解提 PR 的流程，可以阅读：

[拉取请求 - mmcv 1.4.4 文档](https://mmcv.readthedocs.io/zh-cn/latest/community/pr.html)

## 更新文档

在 docs/en/understand_mmcv/ops.md 以及 docs/zh_cn/understand_mmcv/ops.md 里添加 TensorAdd，该文档用于维护 MMCV 已支持的算子。

## 算子实现

在这篇文章里我们提供了CPU 算子和 CUDA 算子的实现方法，您可以选择只提供 CPU 算子或者只提供 CUDA 算子或者两种算子都提供。

### 提供算子 C++ 接口
在 mmcv/ops/csrc/pytorch/ 目录添加 tensor_add.cpp 文件。

```c++
// Copyright(c) OpenMMLab.All rights reserved. 
#include "pytorch_cpp_helper.hpp" 
#include "pytorch_device_registry.hpp" 
 
void tensor_add_impl(const Tensor input1, const Tensor input2, Tensor output) { 
  DISPATCH_DEVICE_IMPL(tensor_add_impl, input1, input2, output); 
} 
 
void tensor_add(const Tensor input1, const Tensor input2, Tensor output) { 
  tensor_add_impl(input1, input2, output); 
} 
```
 
tensor_add 是算子在 C++ 层的接口，而 tensor_add_impl 中的 DISPATCH_DEVICE_IMPL 宏会根据 Tensor 参数的设备类型自动选择 CPU 或 CUDA 的算子实现。DISPATCH_DEVICE_IMPL 宏依赖于REGISTER_DEVICE_IMPL 宏，我们会在下面的 CPU 算子实现和 CUDA 算子实现提到。 DISPATCH_DEVICE_IMPL 和 REGISTER_DEVICE_IMPL 都用于算子分发，更多细节见 [兼容性文档](https://mmcv.readthedocs.io/zh-cn/latest/compatibility.html#v1-3-18)，[PR-1463]https://github.com/open-mmlab/mmcv/pull/1463) 以及 [Dispatcher 机制解析](https://zhuanlan.zhihu.com/p/451671838)。

### CPU 算子实现

在 mmcv/ops/csrc/pytorch/cpu/ 目录添加 tensor_add_cpu.cpp 文件。

```c++
// Copyright(c) OpenMMLab.All rights reserved. 
#include "pytorch_cpp_helper.hpp" 
#include "pytorch_device_registry.hpp" 
 
template <typename T> 
void tensor_add_cpu_kernel(int N, const T* input1, const T* input2, T* output) { 
  for (int i = 0; i < N; i++) { 
 output[i] = input1[i] + input2[i]; 
  } 
} 
 
void TensorAddCPUKernelLaucher(const Tensor input1, const Tensor input2, 
                         Tensor output) { 
  int N = input1.size(0); 
  AT_DISPATCH_FLOATING_TYPES_AND_HALF( 
 input1.scalar_type(), "tensor_add_cpu_kernel", [&] { 
        tensor_add_cpu_kernel<scalar_t>(N, input1.data_ptr<scalar_t>(), 
 input2.data_ptr<scalar_t>(), 
 output.data_ptr<scalar_t>()); 
      }); 
} 
 
void tensor_add_cpu(const Tensor input1, const Tensor input2, Tensor output) { 
  TensorAddCPUKernelLaucher(input1, input2, output); 
} 
void tensor_add_impl(const Tensor input1, const Tensor input2, Tensor output); 
 
REGISTER_DEVICE_IMPL(tensor_add_impl, CPU, tensor_add_cpu); 
```

我们看到最后一行 REGISTER_DEVICE_IMPL 宏将 tensor_add_impl 和 tensor_add_cpu 绑定在一起。 tensor_add_cpu 调用 TensorAddCPUKernelLaucher，TensorAddCPUKernelLaucher 启动 tensor_add_cpu_kernel 完成算子的计算。

### CUDA 算子实现

CUDA 算子的调用过程和 CPU 算子类似，但是在代码安排上略有不同，比如很多 CUDA Kernel 被放在 mmcv/ops/csrc/common/cuda/ 目录，因为该目录负责管理后端无关可共享的代码（cuda kernel，mlu kernel等）。

#### 算子绑定

在 mmcv/ops/csrc/pytorch/cuda/cudabind.cpp 里添加 TensorAdd 的 CUDA 算子绑定。

```c++
// Copyright (c) OpenMMLab. All rights reserved. 
... 
void TensorAddCUDAKernelLauncher(const Tensor input1, const Tensor input2, 
                                 const Tensor output); 
 
void tensor_add_cuda(const Tensor input1, const Tensor input2, Tensor output) { 
  TensorAddCUDAKernelLauncher(input1, input2, output); 
} 
 
void tensor_add_impl(const Tensor input1, const Tensor input2, Tensor output); 
 
REGISTER_DEVICE_IMPL(tensor_add_impl, CUDA, tensor_add_cuda); 
```

我们看到最后一行 REGISTER_DEVICE_IMPL 宏将 tensor_add_impl 和 tensor_add_cuda 绑定在一起。 tensor_add_cuda 调用 TensorAddCUDAKernelLaucher。

#### KernelLaucher

在 mmcv/ops/csrc/pytorch/cuda/ 目录添加 tensor_add_cuda.cu 文件。

```c++
// Copyright (c) OpenMMLab. All rights reserved. 
#include <torch/types.h> 
 
#include "pytorch_cuda_helper.hpp" 
#include "tensor_add_cuda_kernel.cuh" 
 
void TensorAddCUDAKernelLauncher(const Tensor input1, const Tensor input2, 
                                 const Tensor output) { 
  int N = input1.size(0); 
  at::cuda::CUDAGuard device_guard(input1.device()); 
  cudaStream_t stream = at::cuda::getCurrentCUDAStream(); 
 
  AT_DISPATCH_FLOATING_TYPES_AND_HALF( 
      input1.scalar_type(), "tensor_add_cuda_kernel", ([&] { 
        dim3 blocks(GET_BLOCKS(N)); 
        dim3 threads(THREADS_PER_BLOCK); 
        tensor_add_cuda_kernel<<<blocks, threads, 0, stream>>>( 
            N, input1.data_ptr<scalar_t>(), input2.data_ptr<scalar_t>(), 
            output.data_ptr<scalar_t>()); 
      })); 
 
  AT_CUDA_CHECK(cudaGetLastError()); 
} 
```

TensorAddCUDAKernelLauncher 会启动 tensor_add_cuda_kernel 完成算子的具体操作。其中使用 AT_DISPATCH_FLOATING_TYPES_AND_HALF 宏启动 CUDA Kernel ，该宏内部包装了一个 switch 语句来完成针对张量类型的分派，更多这类宏可见：https://github.com/pytorch/pytorch/blob/HEAD/aten/src/ATen/Dispatch.h#L262。

#### Kernel

在 mmcv/ops/csrc/common/cuda/ 下添加 tensor_add_cuda_kernel.cuh 。

```c++
// Copyright (c) OpenMMLab. All rights reserved. 
#ifndef TENSOR_ADD_CUDA_KERNEL_CUH 
#define TENSOR_ADD_CUDA_KERNEL_CUH 
 
#ifdef MMCV_USE_PARROTS 
#include "parrots_cuda_helper.hpp" 
#else 
#include "pytorch_cuda_helper.hpp" 
#endif 
 
template <typename T> 
__global__ void tensor_add_cuda_kernel(const int N, const T* input1, 
                                       const T* input2, T* output) { 
  CUDA_1D_KERNEL_LOOP(i, N) { output[i] = input1[i] + input2[i]; } 
} 
#endif  // TENSOR_ADD_CUDA_KERNEL_CUH 
```

在这里实现了算子的具体操作。其中 CUDA_1D_KERNEL_LOOP 是 MMCV 提供的一个简写 Kernel Loop 的宏，更多这类宏可见：https://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/common/cuda/common_cuda_helper.hpp#L6。

## 提供 Python 接口

在完成 C++/CUDA 的算子后，我们需要在 mmcv/ops/csrc/pytorch/pybind.cpp 里实现 C++ 接口和 Python 接口的绑定，从而提供一个 Python 可以调用的接口。

```c++
// Copyright (c) OpenMMLab. All rights reserved. 
... 
void tensor_add(const Tensor input1, const Tensor input2, Tensor output); 
 
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {   
  ... 
  m.def("tensor_add", &tensor_add, "tensor_add", py::arg("input1"), 
        py::arg("input2"), py::arg("output")); 
} 
```

## Python 调用算子

在 mmcv/ops/ 下添加 tensor_add.py 。

```python
# Copyright (c) OpenMMLab. All rights reserved. 
import torch 
from torch.autograd import Function 
from torch.autograd.function import once_differentiable 
 
from ..utils import ext_loader 
 
ext_module = ext_loader.load_ext('_ext', ['tensor_add']) 
 
 
class TensorAdd(Function): 
 
    @staticmethod 
    def forward(ctx, input1: torch.Tensor, 
                input2: torch.Tensor) -> torch.Tensor: 
        """Add two tensor. 
 
        Args: 
            input1 (torch.Tensor): shape (N). 
            input2 (torch.Tensor): shape (N). 
 
        Returns: 
            torch.Tensor: shape (N), tensor of input1 + input2. 
        """ 
        assert input1.is_contiguous() and input2.is_contiguous() 
        assert input1.dim() == 1 and input2.dim() == 1 
        assert input1.size(0) == input2.size(0) 
        output = torch.zeros( 
            input1.size(0), dtype=input1.dtype, device=input1.device) 
        ext_module.tensor_add(input1, input2, output) 
        return output 
 
    @staticmethod 
    @once_differentiable 
    def backward(ctx, grad_output): 
        return grad_output, grad_output 
 
 
tensor_add = TensorAdd.apply 
```

在 mmcv/ops/__init__.py 文件添加对外的接口。

```python
# Copyright (c) OpenMMLab. All rights reserved. 
... 
from .tensor_add import tensor_add 
 
__all__ = [ 
    ...,  
    'tensor_add' 
] 
```

## 编译 MMCV

参考 [从源码编译 MMCV](https://mmcv.readthedocs.io/zh-cn/latest/get_started/build.html) 的步骤重新编译 MMCV，更多关于 C++/CUDA 算子实现和调用全流程的内容可见：[PyTorch 源码解读之 cpp_extension](https://zhuanlan.zhihu.com/p/348555597)。

## 添加单元测试

在 tests/test_ops/ 下添加 test_tensor_add.py 。

```python
import numpy as np 
import pytest 
import torch 
 
from mmcv.ops import tensor_add 
 
 
@pytest.mark.parametrize('device', [ 
    'cpu', 
    pytest.param( 
        'cuda', 
        marks=pytest.mark.skipif( 
            not torch.cuda.is_available(), reason='requires CUDA support')) 
]) 
@pytest.mark.parametrize('dtype', [torch.float, torch.half]) 
def test_tensor_add(device, dtype): 
    n = 1024 * 1024 
    input1 = torch.rand(n).type(dtype).to(device).requires_grad_() 
    input2 = torch.rand(n).type(dtype).to(device).requires_grad_() 
    expected_output = (input1 + input2).cpu() 
    output = tensor_add(input1, input2) 
    output.backward(torch.ones_like(output)) 
    assert np.allclose( 
        output.detach().cpu(), expected_output.detach(), atol=1e-4) 
    assert np.allclose(input1.grad.cpu().detach(), 1, atol=1e-4) 
```

在终端通过 pytest tests/test_ops/test_tensor_add.py 测试 tensor_add 是否正确。

## 需要注意的点

1. 文件需要添加 copyright。
2. DISPATCH_DEVICE_IMPL 接口参数至少要有一个是 Tensor，否则无法根据设备类型做算子分发。
3. 目录安排、代码风格和命名规范等需符合 MMCV 算子的规范，例如 tensor_add_cpu_kernel 不能写成 tensor_add_kernel、tensor_add_cpukernel 等其他形式。
4. 如果算子需要反向传播，请编写反向传播算子并根据 自定义 C++ 和 CUDA 扩展 组织 Python 代码。

## 函数调用流程图

横实线区别 Python 层和 C++ 层，两者之间通过 pybind11 绑定； 竖实线区别 C++ 算子和 CUDA 算子，算子的分发由 DISPATCH_DEVICE_IMPL 决定。

![fig](https://pic2.zhimg.com/80/v2-aa07fe428e464f26f17820c37b301089_720w.webp)

## 总结

希望本篇文章让您更为深入地了解了如何在 MMCV 中添加自定义算子。